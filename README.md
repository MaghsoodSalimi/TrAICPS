<!--
<meta charset="utf-8">
<meta http-equiv="refresh" content="0; URL=https://HCC2023.github.io/{{ https://zmoez.github.io/HCC2023.github.io/ }}">
<meta http-equiv="refresh" content="0; URL=https://HCC2023.github.io">
-->


<!--
<img src="./HCC2023-2.png" align="left" height="400" width="1024" >

<img src="./HCC2023-3.png" align="left" height="400" width="1024" >
-->

<img src="./HCC2023-MDU.jpg" align="left" height="400" width="1024" >

<!-- <a href="url"><img src="./IMG_0898.jpg" align="left" height="48" width="48" opacity="0.5";></a> -->





<!--
# HCC2023

**Workshop on Human Centric Cybersecurity HCC2023**

**[Mälardalen University](https://www.mdu.se/en/malardalen-university), Västerås, Sweden**

**August 24th, 2023**
-->

## About
We are organizing the first workshop on human-centric cybersecurity at MDU. As in many other areas of engineering the term “human-centric” is becoming a mean to stress the importance of involving end-users and other stakeholders into the design processes, system development, and all the way to testing, validation and verification.
We have invited experts in the field from all over the world giving talks on various aspects of human-centric cybersecurity and safety, and in general human factor in our digital world.
The goal of the workshop is to use the knowledge and experience of the experts in the field and strengthen our knowledge on human-centric research.



## Invited Speakers
<!-- FIXME Check the links -->

**[Edward Lee](https://www2.eecs.berkeley.edu/Faculty/Homepages/lee.html), UC Berkeley, USA**

**[Hazem Torfah](https://www.chalmers.se/en/persons/hazemto/), Chalmers University of Technology, Sweden**

**[Marjan Sirjani](https://www.es.mdu.se/staff/3242-Marjan_Sirjani), Mälardalen University, Sweden** 

**[Matthias Wagner](https://portal.research.lu.se/sv/persons/matthias-wagner), Lund University, Sweden**

**[Shiva Sander Tavallaey](https://www.kth.se/profile/tssander), ABB and KTH, Sweden**

**[Claire Pagetti](https://www.onera.fr/en/staff/claire-pagetti), ONERA, France**

**[Pierluigi Nuzzo](https://www2.eecs.berkeley.edu/Faculty/Homepages/pnuzzo.html), UC Berkeley, USA**





## Invited Talks



## Edward A. Lee  
### Title: Certainty or Intelligence: Pick One!  

### Abstract  
Cyber-physical systems are systems that integrate software and networking with physical components that sense and actuate in the physical world. Traditionally, to make them trustworthy, engineers strive to make their behaviors predictable, repeatable, and provably safe. While security is essential to achieving these objectives, it is by no means sufficient. Many technical challenges arise with sporadic connectivity, real-time behaviors, and handling of inevitable faults.  

While the integration of machine-learning-based AI systems holds promise of being able to help deal with many of the security and other challenges, they simultaneously make behaviors less predictable and repeatable. In this talk, I postulate that the certainty that is traditionally achieved with rigorous engineering methods may be fundamentally incompatible with integrating intelligence into our systems. The question then becomes, how do we get the advantages of intelligence with acceptable risk?  

### Bio  
Edward A. Lee is Professor of the Graduate School and Distinguished Professor Emeritus in Electrical Engineering and Computer Sciences (EECS) at the University of California at Berkeley, where he has been on the faculty since 1986. He is the author of seven books, some with several editions, including two for a general audience, and hundreds of papers and technical reports.  

Lee has delivered more than 200 keynote and other invited talks at venues worldwide and has graduated 40 PhD students. His research group studies cyber-physical systems, which integrate physical dynamics with software and networks. His focus is on the use of deterministic models as a central part of the engineering toolkit for such systems. He is the director of iCyPhy, the Berkeley Industrial Cyber-Physical Systems Research Center.  

From 2005-2008, he served as Chair of the EE Division and then Chair of the EECS Department at UC Berkeley. He has led the development of several influential open-source software packages, notably Ptolemy and Lingua Franca.  

He received his BS degree in 1979 from Yale University, with a double major in Computer Science and Engineering and Applied Science, an SM degree in EECS from MIT in 1981, and a Ph.D. in EECS from UC Berkeley in 1986. From 1979 to 1982, he was a member of technical staff at Bell Labs in Holmdel, New Jersey, in the Advanced Data Communications Laboratory.  

He is a co-founder of BDTI, Inc., where he is currently a Senior Technical Advisor, and has consulted for a number of other companies. He is a Fellow of the IEEE, was an NSF Presidential Young Investigator, won the 1997 Frederick Emmons Terman Award for Engineering Education, received the 2016 Outstanding Technical Achievement and Leadership Award from the IEEE Technical Committee on Real-Time Systems (TCRTS), the 2018 Berkeley Citation, the 2019 IEEE Technical Committee on Cyber-Physical Systems (TCCPS) Technical Achievement Award, the 2022 European Design and Automation Association (EDAA) Achievement Award, the 2022 ACM SIGBED Technical Achievement Award, and an Honorary Doctorate in Computer Science from the Technical University of Vienna.  


## Hazem Torfah  
### Title: Learning and Monitoring the Operational Design Domain for AI-Based Autonomy  

### Abstract  
The deployment of autonomous systems in complex environments has increased significantly in recent years, with many relying on machine learning (ML) components for critical decision-making tasks. However, ML models are inherently brittle and prone to failures that can compromise system safety. This highlights the need for a systematic methodology to identify the conditions under which autonomy pipeline components may fail at design time and to detect such failures at runtime.  

This talk introduces a novel safety assurance approach for autonomous systems based on runtime verification. Runtime verification involves extracting information from a system at runtime and evaluating it to determine whether an execution satisfies or violates a given (safety) property. We will present recent advancements in runtime verification methods for capturing operational design domains (ODD), the conditions under which a system or its components are designed to operate safely. We will introduce a formalization for monitorable ODDs and discuss challenges in learning monitors for ODDs in noisy and unpredictable environments.  

### Bio  
*No bio provided.*  




<h3>Gordana Dodig-Crnkovic</h3>
<b>Title: Robots Ethical by Design in the Perspective of Digital Humanism</b>
<br /> <br /> 
<b>Abstract</b>: This keynote talk will explore the integration of ethics into the design of advanced robots and softbots through artificial morality. As these agents are set to evolve, their ethical foundation will need to align with their increasing intelligence and autonomy. The talk will introduce functional artificial moral responsibility, emphasizing a holistic approach that will merge technical, human, and societal dimensions. The significance of foresight in ethics and speculative design to address potential future challenges, especially in solving typical complex "white water problems," will be highlighted. The session will offer ethical and social perspectives from Digital Humanism, arguing that a technology-driven future must be inclusive and humane. Finally, some of the current activities of the Digital Humanism movement will be outlined.
<br /> <br /> 
<b>Bio</b>: Gordana Dodig-Crnković is a Professor of Computer Science at Mälardalen University and a Professor in Interaction Design at Chalmers University of Technology in Sweden. She holds Ph.D. in Physics and Computer Science. Her current research encompasses two focus areas. The first explores the physical mechanisms of computation and the interplay between morphological computation, information, and cognition. The second centers on the ethical and social implications of computing. Dodig-Crnković’s interest in the value-driven aspects of technology has led her to teach ethics to technology students for many years. This focus is attuned to new and emerging technologies and has resulted in a series of articles addressing the ethical facets of AI, robotics, and autonomous vehicles. Dodig-Crnković is actively involved in several committees and boards, including the Chalmers AI Ethics Committee, Informatics Europe Board, the European Network for Gender Balance in Informatics (EUGAIN), and the Inclusion4EU project.
<br /> 
<br /> 
<h3>Simon Parkin</h3> 
<b>Title: Refining the Blunt Instruments of Cyber-Risk Management</b>
<br /> <br /> 
<b>Abstract</b>: Well-meaning cybersecurity risk owners will deploy controls and countermeasures in an effort to manage the risks they see affecting their services or systems. These controls and countermeasures may produce unintended, negative harms themselves, adversely affecting user behaviour, user inclusion, or the infrastructure itself. Here I will first describe a framework for exploring the potential unintended harms of security and privacy controls, informed by a range of case studies. I will then describe subsequent work to explore how existing cyber-risk management approaches can be adapted to be less ‘blunt’ and more precise, to preserve legitimate behaviours while preventing malicious behaviours within a managed system. This leads to consideration of challenges in securing a system against identifiable risks, while ensuring that it is usable - and ultimately, accessible - for intended users.
<br /> <br /> 
<b>Bio</b>: Simon is an Assistant Professor in the Cybersecurity group in the Technology, Policy, and Management (TPM) faculty at the Delft University of Technology (TU Delft, Netherlands). His specialization is in human-centred security: usability and perceptions of security-related technologies, security behaviour change, security economics, and decision-making in security technology management, support, and policy. Current research includes: examining how best to position security and remediation support for users of consumer IoT devices; practitioner experiences and decisions in patching of IT systems in complex organizations, and; multi-stakeholder perspectives on the management of employee-facing security in organizations.
<br /> 
<br /> 
<h3>Gabriele Lenzini</h3>
<b>Title: Sociotechnical Cybersecurity: an overview, with reference to end-to-end encryption and threat intelligence</b>
<br /> <br /> 
<b>Abstract</b>: On this talk I will give an overview of the research on sociotechnical security taking from my personal experience in running an international workshop on the subject (STAST). I will then move to explain how today sociotechnical security applies in the forthcoming topic of cybersecurity, and I will share thoughts and some insights from my on research on combining formal modelling of secure solutions with insights coming from research investigation on user experience in security. I will refer to two particular use cases: end-to-end encryption and secure cyber threats data sharing.
<br /> <br /> 
<b>Bio</b>: Lenzini Gabriele, holds a PhD in Computer Security from the University of Twente, in the Netherlands. He is Associate Professor at the University of Luxembourg (UL), and head of the Sociotechnical Cybersecurity research group at the UL’s center for Security Reliability and Trust (SnT).
Lenzini’s research stems from formal approach to modelling and analysis of security solutions but extends to situations where the design or the analysis of security solutions intertwine with human and social aspects. Thus, it also explores questions regarding how security related with users as individuals (usability and user experience) or as collective (people’s trust depending on properties like fairness and transparency). Lenzini is UL member in Informatics Europe, ECSO, and the IFIP WG on Human Aspects of Information Security and Assurance. He is vice-chairs of the UL Ethical Review Board.
<br /> 
<br /> 
<h3>Paolo Masci</h3>
<b>Title: A human-centric hazard analysis method for identifying design anomalies in safety critical systems</b>
<br /> <br /> 
<b>Abstract</b>: In this talk I will present a hazard analysis method for the systematic identification of use-related hazards in safety critical systems. The method builds on usability engineering standards and an existing hazard analysis method. The method has been employed successfully in a range of different case studies, to identify potential design anomalies in user interface software. Examples based on medical systems will be presented, including an application to next-gen interoperable medical devices.
<br /> <br /> 
<b>Bio</b>: 
Paolo Masci is an Associate Principal Research Scientist with Analytical Mechanics Associates (AMA) at NASA Langley Research Center (NASA LaRC). His expertise is on modeling and verification of human-machine interfaces in safety-critical systems. At NASA LaRC, Paolo is working on topics related to Air Mobility (UAM) and Advanced Air Mobility (AAM). Prior to joining NASA LaRC, Paolo developed his career in various universities and worked in close collaboration with the FDA Center for Devices and Radiological Health to carry out applied research on medical devices.
<br /> 
<br /> 

 

 
## Schedule

**09:00 - 10:00** &nbsp; &nbsp; **Edward Lee:** Certainty or Intelligence: Pick One!

**10:00 - 10:15** &nbsp; &nbsp; **Break**  

**10:15 - 11:00** &nbsp; &nbsp; **Hazem Torfah:** Formal verification and monitoring in autonomous driving  

**11:00 - 12:00** &nbsp; &nbsp; **Marjan Sirjani and team:** Runtime monitoring and shielding AI-enabled CPS  

**12:00 - 13:00** &nbsp; &nbsp; **Lunch Break***  

**13:00 - 13:40** &nbsp; &nbsp; **Matthias Wagner:** The AI Act’s (AIA) requirements for high-risk AI systems and the effect on modern software systems and industry  

**13:40 - 14:10** &nbsp; &nbsp; **Shiva Sander Tavallaey:** Trustworthy AI at ABB  

**14:10 - 15:00** &nbsp; &nbsp; **Panel**  

**15:00 - 15:15** &nbsp; &nbsp; **Break**  

**15:15 - 16:00** &nbsp; &nbsp; **Claire Pagetti (Online):** Formal verification and certified machine learning and Airbus  

**16:00 - 17:00** &nbsp; &nbsp; **Pierluigi Nuzzo (Online):** Formal verification and AI in CPS  



## Place

**Alpha** conference room, First floor of U building, MDU, Västerås. 

Please note that this is a hybrid event and you can also follow online, but a reliable connection is not guaranteed.

## Online
**Zoom:** TODO- put link

## Organizers
[Marjan Sirjani](http://www.es.mdu.se/staff/3242-Marjan_Sirjani)

TODO: any1 else?


## Contact Info
Marjan Sirjani

Email: marjan.sirjani@mdu.se

Room: U1-066C

Phone: +46736620517

